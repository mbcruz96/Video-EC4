#Video-EC4
The inherent properties of egocentric video pose a significant challenge for traditional LVLMs, hampering their comprehension. Challenges such as unusual viewpoints, 
rapid camera movement, and the strong focus on hands and objects in the setting of action requires the model to go beyond the conventional video comprehension approach 
of merely depending on object recognition. These tasks require complex reasoning abilities regardingcontext, spatial dynamics, and the holistic motivations behind the 
captured scenes.To close this gap, this paper introduces Video-EC4. We aim to benchmark existing state-of-the-art LVLMs(specifically Video-ChatGPT and Video-LLaVA) on the
EgoSchema dataset and then finetuned the models on Ego4D to enhance the performance on such egocentric data. Subsequently, the models were benchmarked on EgoSchema once 
more to compare the performance of the fine-tuned models.
